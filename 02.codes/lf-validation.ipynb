{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 20:46:41.847217: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-11 20:46:41.847311: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-11 20:46:41.849204: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import MeanSquaredError, Huber,MeanAbsoluteError\n",
    "from keras.layers import Dense, LSTM, Reshape,Dropout,Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "from vmdpy import VMD\n",
    "\n",
    "from typing import List as List, Tuple as Tuple, Dict as Dict,Optional as Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temperature</th>\n",
       "      <th>total_load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4344</th>\n",
       "      <td>2022-07-01 00:00:00+00:00</td>\n",
       "      <td>18.143443</td>\n",
       "      <td>61874.013017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4345</th>\n",
       "      <td>2022-07-01 01:00:00+00:00</td>\n",
       "      <td>17.446575</td>\n",
       "      <td>59042.531005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>2022-07-01 02:00:00+00:00</td>\n",
       "      <td>16.976438</td>\n",
       "      <td>57149.292992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>2022-07-01 03:00:00+00:00</td>\n",
       "      <td>16.481543</td>\n",
       "      <td>56519.531943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>2022-07-01 04:00:00+00:00</td>\n",
       "      <td>16.140110</td>\n",
       "      <td>56818.247011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      datetime  temperature    total_load\n",
       "4344 2022-07-01 00:00:00+00:00    18.143443  61874.013017\n",
       "4345 2022-07-01 01:00:00+00:00    17.446575  59042.531005\n",
       "4346 2022-07-01 02:00:00+00:00    16.976438  57149.292992\n",
       "4347 2022-07-01 03:00:00+00:00    16.481543  56519.531943\n",
       "4348 2022-07-01 04:00:00+00:00    16.140110  56818.247011"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_load = pd.read_csv(\"/mnt/e/github/load-forecast/01.database/processed/load/2022_2023_load_processed.csv\", sep = \",\", encoding = 'latin-1', index_col=0)\n",
    "df_temp = pd.read_csv(\"/mnt/e/github/load-forecast/01.database/processed/temperature/2022_2023_temperature_processed.csv\", sep = ',',encoding = 'latin-1', index_col=0)\n",
    "\n",
    "df_temp['datetime'] = pd.to_datetime(df_temp['datetime'])\n",
    "df_load['datetime'] = pd.to_datetime(df_load['datetime']).dt.tz_localize('UTC')\n",
    "\n",
    "df_raw = pd.merge(df_temp,df_load, on='datetime', how='inner')\n",
    "\n",
    "del df_load\n",
    "del df_temp\n",
    "\n",
    "df_raw = df_raw.iloc[4344:]\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ciclical_time_encoding(df:pd.DataFrame)->pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Encodes datetime values in a DataFrame using cyclic encoding.\n",
    "\n",
    "    Args:\n",
    "        - df: DataFrame containing a 'datetime' column to be cyclically encoded.\n",
    "\n",
    "    Returns:\n",
    "        - DataFrame with additional columns representing the cyclically encoded datetime values.\n",
    "    \"\"\"\n",
    "\n",
    "    #Ciclical encoding datetime object:\n",
    "    day = 1\n",
    "    weekly = day*7\n",
    "    year = day*365\n",
    "\n",
    "    #Convert into an number\n",
    "    timestamp_s = df['datetime'].map(datetime.timestamp)\n",
    "\n",
    "    df = df.assign(year_sin = (np.sin(timestamp_s * (2*np.pi/year))).values)\n",
    "    df = df.assign(year_cos = (np.cos(timestamp_s * (2*np.pi/year))).values)\n",
    "\n",
    "    df = df.assign(daily_sin = (np.sin(timestamp_s * (2*np.pi/day))).values)\n",
    "    df = df.assign(daily_cos = (np.cos(timestamp_s * (2*np.pi/day))).values)\n",
    "\n",
    "    df = df.assign(weekly_sin = (np.sin(timestamp_s * (2*np.pi/weekly))).values)\n",
    "    df = df.assign(weekly_cos = (np.cos(timestamp_s * (2*np.pi/weekly))).values)\n",
    "    \n",
    "    df.reset_index(inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_remove_datetime(df:pd.DataFrame)-> List[datetime]:\n",
    "\n",
    "    '''\n",
    "    - Args:\n",
    "        - df: Dataframe to remove datetime column\n",
    "    - Returns:\n",
    "        - datetime_rage: Removed column as a List\n",
    "    '''\n",
    "   \n",
    "    datetime_range = df['datetime'].to_list()\n",
    "    df.drop(columns = ['datetime'], inplace = True)\n",
    "    \n",
    "    return datetime_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_series_vmd(df:pd.DataFrame,timeseries:str, k_nmodes:int)-> pd.DataFrame:\n",
    "    '''\n",
    "\n",
    "    Decomposes a time series using Variational Mode Decomposition (VMD).\n",
    "    \n",
    "    - Args:\n",
    "        - df: Dataframe with the timeseries that will be decomposed\n",
    "        - timeseres: Dataframe column that will go trought the vmd process\n",
    "        - k_nmodes: n decomposed series\n",
    "    - Returns:\n",
    "        - df: dataframe with new decomposed columns\n",
    "    '''\n",
    "\n",
    "    print(f\"Decomposing {timeseries} in {k_nmodes} nmodes\")\n",
    "    \n",
    "    timeseries = df[timeseries].to_list()\n",
    "\n",
    "    alpha = 2000     # moderate bandwidth constraint  \n",
    "    tau = 0           # noise-tolerance (no strict fidelity enforcement)  \n",
    "    k_nmodes = k_nmodes           # n modes  \n",
    "    DC = 0             # no DC part imposed  \n",
    "    init = 0           # initialize omegas uniformly  \n",
    "    tol = 1e-6\n",
    "    u, u_hat, omega = VMD(timeseries,alpha,tau,k_nmodes,DC,init,tol)\n",
    "\n",
    "    label_columns = []\n",
    "\n",
    "    for i in range(len(u)):\n",
    "        col_name = 'series_dec_' + str(i+1)\n",
    "        df[col_name] = u[i]  \n",
    "        label_columns.append('series_dec_'+str(i+1))\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_scal_df(df:pd.DataFrame)-> Tuple[Dict[str,pd.DataFrame],Tuple[pd.Series,pd.Series]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Splits the input DataFrame into training, validation, and test sets, scales the datasets using Min-Max scaling,\n",
    "    and returns the scaled DataFrames along with the min-max values used for scaling.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame to be split and scaled.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing:\n",
    "        - A dictionary with scaled DataFrams\n",
    "        - A tuple containing min and max values for each feature used during scaling.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(df)\n",
    "    # Split 70:20:10 (train:validation:test)\n",
    "    train_df = df[0:int(n*0.7)]\n",
    "    val_df = df[int(n*0.7):int(n*0.9)]\n",
    "    test_df = df[int(n*0.9):]\n",
    "\n",
    "    #retrieve the max and min values of each feature. This will be used to bring those values back to the original\n",
    "    #dimesion after scalling it to train the model\n",
    "    min_values = train_df.min()\n",
    "    max_values = train_df.max()\n",
    "\n",
    "    #Scaling the dataframes to train\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_df)\n",
    "\n",
    "    scal_df = df.copy()\n",
    "    scal_df[scal_df.columns] = scaler.transform(df[df.columns])\n",
    "\n",
    "    scal_train_df = train_df.copy()\n",
    "    scal_train_df[scal_train_df.columns] = scaler.transform(train_df[train_df.columns])\n",
    "\n",
    "    scal_val_df = val_df.copy()\n",
    "    scal_val_df[scal_val_df.columns] = scaler.transform(val_df[val_df.columns])\n",
    "\n",
    "    scal_test_df = test_df.copy()\n",
    "    scal_test_df[scal_test_df.columns] = scaler.transform(test_df[test_df.columns])\n",
    "\n",
    "    dict_scal_df = {\"train_df\":scal_train_df,\n",
    "                    \"val_df\":scal_val_df,\n",
    "                    \"test_df\":scal_test_df}\n",
    "\n",
    "    min_max_values = (min_values,max_values)\n",
    "\n",
    "    return scal_df,min_max_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_and_plot (df,min_values,max_values,model,input_width,forecast_days,steps_per_day,\n",
    "                         label_columns=label_columns):\n",
    "\n",
    "    #Convert varibles from days to timesteps.\n",
    "    \n",
    "    input_width = input_width*steps_per_day\n",
    "    forecast_width = forecast_days*steps_per_day\n",
    "\n",
    "    #Input Period\n",
    "    start_input = 0\n",
    "    end_input = input_width\n",
    "\n",
    "    #Forecast/Label period\n",
    "    start_forecast = end_input\n",
    "    end_forecast = start_forecast + forecast_width\n",
    "    \n",
    "    now = datetime.datetime(2023,7,3,17,0,0)\n",
    "\n",
    "    labels = [now]\n",
    "\n",
    "    for time in range(input_width + forecast_width):\n",
    "        \n",
    "        now = now+timedelta(hours=1) \n",
    "        labels.append(now)\n",
    "\n",
    "    #Registring dates as labels to the plot\n",
    "    inputs_labels = labels[start_input:end_input]\n",
    "    predictions_labels = labels[start_forecast:end_forecast]\n",
    "\n",
    "    #Converting inputs to tensor\n",
    "    inputs = tf.convert_to_tensor(df[start_input:end_input])\n",
    "    inputs_to_model = tf.expand_dims(inputs,axis=0)\n",
    "\n",
    "    #Converting real values to tensor\n",
    "    real_values = tf.convert_to_tensor(df[start_forecast:end_forecast])\n",
    "    \n",
    "\n",
    "    #slicing inputs to retrieve only the decomposed windspeeds\n",
    "    sliced_inputs = inputs[:,-len(label_columns):]\n",
    "   \n",
    "\n",
    "    #Scale back to the original value range\n",
    "    original_scale_inputs = scale_back(sliced_inputs,min_values,max_values,label_columns)\n",
    "    \n",
    "    #Sum across all decomposed values\n",
    "    summed_original_scale_inputs = tf.math.reduce_sum(original_scale_inputs,axis=1,keepdims=False,name=None)\n",
    "    \n",
    "\n",
    "    #Make predictions\n",
    "    predictions = model(inputs_to_model)\n",
    "    predictions = tf.squeeze(predictions,axis=0)\n",
    "\n",
    "    #Scale predictions and sum all decomposed windspeeds\n",
    "    original_scale_predictions = scale_back(predictions,min_values,max_values,label_columns)\n",
    "    summed_original_scale_predictions = tf.math.reduce_sum(original_scale_predictions,axis=1,keepdims=False,name=None)\n",
    "\n",
    "    #Avg calc\n",
    "\n",
    "    predictions_avg = calc_ws_avg(summed_original_scale_predictions,steps_per_day)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=inputs_labels,\n",
    "                            y=summed_original_scale_inputs,\n",
    "                            mode='lines',\n",
    "                            name='inputs',\n",
    "                            line = dict(color='#2E294E',width=4)\n",
    "                            ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=predictions_labels,\n",
    "                            y=summed_original_scale_predictions,\n",
    "                            mode='lines',\n",
    "                            name='predictions',\n",
    "                            line = dict(color='#D90368',width=4)\n",
    "                            ))\n",
    "\n",
    "    x_0 = inputs_labels[-1]\n",
    "    text_position = x_0 \n",
    "    \n",
    "    fig.add_annotation(\n",
    "        showarrow=False,\n",
    "        text = 'Wind speed daily avg (m/s)',\n",
    "        x = text_position - timedelta(days=1),\n",
    "        y= 14,\n",
    "        font = dict(size = 12, color = 'black')\n",
    "\n",
    "    )\n",
    "    \n",
    "    for day in range(forecast_days):\n",
    "        \n",
    "        text_string = 'day_' + str(day+1) + ': pred = '+ str(predictions_avg[day])   \n",
    "                 \n",
    "        fig.add_annotation(\n",
    "            showarrow=False,\n",
    "            text = text_string,\n",
    "            x = text_position + timedelta(hours=12),\n",
    "            y = 14,\n",
    "            font = dict(size = 11, color = 'black'),\n",
    "        )\n",
    "        fig.add_vline(\n",
    "            x = text_position,\n",
    "            line_width = 1.5,\n",
    "            line_dash = 'dash',\n",
    "            line_color = 'black'\n",
    "        )\n",
    "\n",
    "        text_position = text_position + timedelta(days=1)\n",
    "\n",
    "    fig.update_layout(title='[TRS02] Wind speed forecast',\n",
    "                    xaxis_title='',\n",
    "                    yaxis_title='Windspeed (m/s)',\n",
    "                    template = 'plotly_white',\n",
    "                    yaxis = dict(range=[0,15]))\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/e/github/load-forecast/03.results/'\n",
    "\n",
    "model_list = [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file)) and file.endswith(\".keras\")]\n",
    "\n",
    "model_dict = {}\n",
    "temperature = False\n",
    "for model in model_list:\n",
    "    if \"temperature\" in model:\n",
    "        temperature = True\n",
    "    k_nmodes = model.split(\"_\")[-1]\n",
    "    k_nmodes = int(k_nmodes.split(\".\")[0])\n",
    "    model_dict[model] = {\"temperature\":temperature,\n",
    "                         \"k_nmodes\":k_nmodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['load_forecast_knmodes_40.keras', 'load_forecast_with_temperature_knmodes_10.keras', 'load_forecast_with_temperature_knmodes_20.keras'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {'MeanAbsoluteError': MeanAbsoluteError}\n",
    "#lstm_model = load_model(model_path, custom_objects=custom_objects)\n",
    "\n",
    "model = \"/mnt/e/github/load-forecast/03.results/load_forecast_with_temperature_knmodes_10.keras\"\n",
    "#model_path = os.path.join(path,model)\n",
    "lstm_model = load_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing total_load in 10 nmodes\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lsmt_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m scal_df, min_max_values \u001b[38;5;241m=\u001b[39m split_and_scal_df(df \u001b[38;5;241m=\u001b[39m df)\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlsmt_model\u001b[49m(scal_df[:\u001b[38;5;241m72\u001b[39m])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lsmt_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "k_nmodes = int(model.split(\"_\")[-1].split(\".\")[0])\n",
    "df = df_raw.copy()\n",
    "\n",
    "df = ciclical_time_encoding(df = df)\n",
    "datetime_range = retrieve_and_remove_datetime(df = df)\n",
    "df = decompose_series_vmd(df = df, timeseries = 'total_load', k_nmodes = k_nmodes)\n",
    "\n",
    "if temperature == False:\n",
    "    df.drop(columns = ['temperature'], inplace = True)\n",
    "\n",
    "total_load = df['total_load'].to_list()\n",
    "df.drop(columns = ['total_load'], inplace = True)\n",
    "df.drop(columns = ['index'], inplace = True)\n",
    "\n",
    "scal_df, min_max_values = split_and_scal_df(df = df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'as_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscal_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m72\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda3.9/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/conda3.9/lib/python3.9/site-packages/keras/src/engine/input_spec.py:307\u001b[0m, in \u001b[0;36mdisplay_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisplay_shape\u001b[39m(shape):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtuple\u001b[39m(\u001b[43mshape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_list\u001b[49m()))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'as_list'"
     ]
    }
   ],
   "source": [
    "outputs = lstm_model(scal_df[:72])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_dict.keys:\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"Initializing model validation\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    model_path = os.path.join(path,model)\n",
    "    lstm_model = load_model(model_path)\n",
    "\n",
    "    print(f\"Model {model} loaded\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    temperature = model['temperature']\n",
    "    k_nmodes = model['k_nmodes']\n",
    "\n",
    "    print(f\"Initializing feature eng.\")\n",
    "\n",
    "    df = df_raw.copy()\n",
    "    df = ciclical_time_encoding(df = df)\n",
    "    datetime_range = retrieve_and_remove_datetime(df = df)\n",
    "    df = decompose_series_vmd(df = df, timeseries = 'total_load', k_nmodes = k_nmodes)\n",
    "\n",
    "    if temperature == False:\n",
    "        df.drop(columns = ['temperature'], inplace = True)\n",
    "    \n",
    "\n",
    "    total_load = df['total_load'].to_list()\n",
    "    df.drop(columns = ['total_load'], inplace = True)\n",
    "    df.drop(columns = ['index'], inplace = True)\n",
    "\n",
    "    scal_df, min_max_values = split_and_scal_df(df = df)\n",
    "\n",
    "    validation_dict = {}\n",
    "\n",
    "    for row in scal_df.index():\n",
    "\n",
    "        input = scal_df[row:row+72]\n",
    "\n",
    "        output = lstm_model(input)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws-forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
