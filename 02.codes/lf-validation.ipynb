{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import MeanSquaredError, Huber,MeanAbsoluteError\n",
    "from keras.layers import Dense, LSTM, Reshape,Dropout,Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "from vmdpy import VMD\n",
    "\n",
    "from typing import List as List, Tuple as Tuple, Dict as Dict,Optional as Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temperature</th>\n",
       "      <th>total_load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4344</th>\n",
       "      <td>2022-07-01 00:00:00+00:00</td>\n",
       "      <td>18.143443</td>\n",
       "      <td>61874.013017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4345</th>\n",
       "      <td>2022-07-01 01:00:00+00:00</td>\n",
       "      <td>17.446575</td>\n",
       "      <td>59042.531005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>2022-07-01 02:00:00+00:00</td>\n",
       "      <td>16.976438</td>\n",
       "      <td>57149.292992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>2022-07-01 03:00:00+00:00</td>\n",
       "      <td>16.481543</td>\n",
       "      <td>56519.531943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>2022-07-01 04:00:00+00:00</td>\n",
       "      <td>16.140110</td>\n",
       "      <td>56818.247011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      datetime  temperature    total_load\n",
       "4344 2022-07-01 00:00:00+00:00    18.143443  61874.013017\n",
       "4345 2022-07-01 01:00:00+00:00    17.446575  59042.531005\n",
       "4346 2022-07-01 02:00:00+00:00    16.976438  57149.292992\n",
       "4347 2022-07-01 03:00:00+00:00    16.481543  56519.531943\n",
       "4348 2022-07-01 04:00:00+00:00    16.140110  56818.247011"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_load = pd.read_csv(\"/mnt/e/github/load-forecast/01.database/processed/load/2022_2023_load_processed.csv\", sep = \",\", encoding = 'latin-1', index_col=0)\n",
    "df_temp = pd.read_csv(\"/mnt/e/github/load-forecast/01.database/processed/temperature/2022_2023_temperature_processed.csv\", sep = ',',encoding = 'latin-1', index_col=0)\n",
    "\n",
    "df_temp['datetime'] = pd.to_datetime(df_temp['datetime'])\n",
    "df_load['datetime'] = pd.to_datetime(df_load['datetime']).dt.tz_localize('UTC')\n",
    "\n",
    "df_raw = pd.merge(df_temp,df_load, on='datetime', how='inner')\n",
    "\n",
    "del df_load\n",
    "del df_temp\n",
    "\n",
    "df_raw = df_raw.iloc[4344:]\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ciclical_time_encoding(df:pd.DataFrame)->pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Encodes datetime values in a DataFrame using cyclic encoding.\n",
    "\n",
    "    Args:\n",
    "        - df: DataFrame containing a 'datetime' column to be cyclically encoded.\n",
    "\n",
    "    Returns:\n",
    "        - DataFrame with additional columns representing the cyclically encoded datetime values.\n",
    "    \"\"\"\n",
    "\n",
    "    #Ciclical encoding datetime object:\n",
    "    day = 1\n",
    "    weekly = day*7\n",
    "    year = day*365\n",
    "\n",
    "    #Convert into an number\n",
    "    timestamp_s = df['datetime'].map(datetime.timestamp)\n",
    "\n",
    "    df = df.assign(year_sin = (np.sin(timestamp_s * (2*np.pi/year))).values)\n",
    "    df = df.assign(year_cos = (np.cos(timestamp_s * (2*np.pi/year))).values)\n",
    "\n",
    "    df = df.assign(daily_sin = (np.sin(timestamp_s * (2*np.pi/day))).values)\n",
    "    df = df.assign(daily_cos = (np.cos(timestamp_s * (2*np.pi/day))).values)\n",
    "\n",
    "    df = df.assign(weekly_sin = (np.sin(timestamp_s * (2*np.pi/weekly))).values)\n",
    "    df = df.assign(weekly_cos = (np.cos(timestamp_s * (2*np.pi/weekly))).values)\n",
    "    \n",
    "    df.reset_index(inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_remove_datetime(df:pd.DataFrame)-> List[datetime]:\n",
    "\n",
    "    '''\n",
    "    - Args:\n",
    "        - df: Dataframe to remove datetime column\n",
    "    - Returns:\n",
    "        - datetime_rage: Removed column as a List\n",
    "    '''\n",
    "   \n",
    "    datetime_range = df['datetime'].to_list()\n",
    "    df.drop(columns = ['datetime'], inplace = True)\n",
    "    \n",
    "    return datetime_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_series_vmd(df:pd.DataFrame,timeseries:str, k_nmodes:int)-> pd.DataFrame:\n",
    "    '''\n",
    "\n",
    "    Decomposes a time series using Variational Mode Decomposition (VMD).\n",
    "    \n",
    "    - Args:\n",
    "        - df: Dataframe with the timeseries that will be decomposed\n",
    "        - timeseres: Dataframe column that will go trought the vmd process\n",
    "        - k_nmodes: n decomposed series\n",
    "    - Returns:\n",
    "        - df: dataframe with new decomposed columns\n",
    "    '''\n",
    "\n",
    "    print(f\"Decomposing {timeseries} in {k_nmodes} nmodes\")\n",
    "    \n",
    "    timeseries = df[timeseries].to_list()\n",
    "\n",
    "    alpha = 2000     # moderate bandwidth constraint  \n",
    "    tau = 0           # noise-tolerance (no strict fidelity enforcement)  \n",
    "    k_nmodes = k_nmodes           # n modes  \n",
    "    DC = 0             # no DC part imposed  \n",
    "    init = 0           # initialize omegas uniformly  \n",
    "    tol = 1e-6\n",
    "    u, u_hat, omega = VMD(timeseries,alpha,tau,k_nmodes,DC,init,tol)\n",
    "\n",
    "    label_columns = []\n",
    "\n",
    "    for i in range(len(u)):\n",
    "        col_name = 'series_dec_' + str(i+1)\n",
    "        df[col_name] = u[i]  \n",
    "        label_columns.append('series_dec_'+str(i+1))\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_scal_df(df:pd.DataFrame)-> Tuple[Dict[str,pd.DataFrame],Tuple[pd.Series,pd.Series]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Splits the input DataFrame into training, validation, and test sets, scales the datasets using Min-Max scaling,\n",
    "    and returns the scaled DataFrames along with the min-max values used for scaling.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame to be split and scaled.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing:\n",
    "        - A dictionary with scaled DataFrams\n",
    "        - A tuple containing min and max values for each feature used during scaling.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(df)\n",
    "    # Split 70:20:10 (train:validation:test)\n",
    "    train_df = df[0:int(n*0.7)]\n",
    "    val_df = df[int(n*0.7):int(n*0.9)]\n",
    "    test_df = df[int(n*0.9):]\n",
    "\n",
    "    #retrieve the max and min values of each feature. This will be used to bring those values back to the original\n",
    "    #dimesion after scalling it to train the model\n",
    "    min_values = train_df.min()\n",
    "    max_values = train_df.max()\n",
    "\n",
    "    #Scaling the dataframes to train\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_df)\n",
    "\n",
    "    scal_df = df.copy()\n",
    "    scal_df[scal_df.columns] = scaler.transform(df[df.columns])\n",
    "\n",
    "    scal_train_df = train_df.copy()\n",
    "    scal_train_df[scal_train_df.columns] = scaler.transform(train_df[train_df.columns])\n",
    "\n",
    "    scal_val_df = val_df.copy()\n",
    "    scal_val_df[scal_val_df.columns] = scaler.transform(val_df[val_df.columns])\n",
    "\n",
    "    scal_test_df = test_df.copy()\n",
    "    scal_test_df[scal_test_df.columns] = scaler.transform(test_df[test_df.columns])\n",
    "\n",
    "    dict_scal_df = {\"train_df\":scal_train_df,\n",
    "                    \"val_df\":scal_val_df,\n",
    "                    \"test_df\":scal_test_df}\n",
    "\n",
    "    min_max_values = (min_values,max_values)\n",
    "\n",
    "    return scal_df,min_max_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_back(scaled_values:tf.Tensor,min_values:pd.Series,max_values:pd.Series,label_columns:List[str],variable = 'timeseries'):\n",
    "\n",
    "        if variable == 'timeseries':       \n",
    "                min_values = min_values.tail(len(label_columns)).values\n",
    "                max_values = max_values.tail(len(label_columns)).values\n",
    "                \n",
    "        elif variable == 'val_loss':\n",
    "                min_values = min_values.head(1).values\n",
    "                max_values = max_values.head(1).values\n",
    "\n",
    "        scaled_back = scaled_values * (max_values - min_values) + min_values\n",
    "\n",
    "        return scaled_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_data(\n",
    "                    start_input:int,\n",
    "                    df:pd.DataFrame,\n",
    "                    input_days:int,\n",
    "                    forecast_days:int,\n",
    "                    steps_per_day:int)->tf.Tensor:\n",
    "\n",
    "    #Convert varibles from days to timesteps.\n",
    "    input_hours = input_days*steps_per_day\n",
    "    forecast_hours = forecast_days*steps_per_day\n",
    "\n",
    "    #Input Period\n",
    "    end_input = start_input + input_hours\n",
    "    \n",
    "    #Forecast/Label period\n",
    "    start_forecast = end_input\n",
    "    end_forecast = start_forecast + forecast_hours\n",
    "    \n",
    "    #Converting inputs to tensor\n",
    "    inputs = tf.convert_to_tensor(df[start_input:end_input])\n",
    "    inputs_to_model = tf.expand_dims(inputs,axis=0)\n",
    "    \n",
    "    return(inputs_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forecast(inputs_to_model:tf.Tensor,\n",
    "                   model:Model) -> tf.Tensor:\n",
    "    \n",
    "    #Make predictions\n",
    "    predictions = model(inputs_to_model)\n",
    "    predictions = tf.squeeze(predictions,axis=0)\n",
    "    \n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2997454872.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    hourly_error =\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def process_and_validate_predictions(predictions:tf.Tensor,\n",
    "                                     real_values:List[float],\n",
    "                                     min_values:pd.Series,\n",
    "                                     max_values:pd.Series,\n",
    "                                     label_columns:List[str]):\n",
    "    \n",
    "    #Scale predictions and sum all decomposed series\n",
    "    original_scale_predictions = scale_back(predictions = predictions,\n",
    "                                            min_values = min_values,\n",
    "                                            max_values = max_values,\n",
    "                                            label_columns = label_columns)\n",
    "    \n",
    "    summed_original_scale_predictions = tf.math.reduce_sum(original_scale_predictions,axis=1,keepdims=False,name=None)\n",
    "\n",
    "    hourly_error = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forecast_and_plot (df,min_values,max_values,model,input_width,forecast_days,steps_per_day,\n",
    "                        label_columns=label_columns):\n",
    "\n",
    "    #Convert varibles from days to timesteps.\n",
    "    \n",
    "    input_width = input_width*steps_per_day\n",
    "    forecast_width = forecast_days*steps_per_day\n",
    "\n",
    "    #Input Period\n",
    "    start_input = 0\n",
    "    end_input = input_width\n",
    "\n",
    "    #Forecast/Label period\n",
    "    start_forecast = end_input\n",
    "    end_forecast = start_forecast + forecast_width\n",
    "    \n",
    "    now = datetime.datetime(2023,7,3,17,0,0)\n",
    "\n",
    "    labels = [now]\n",
    "\n",
    "    for time in range(input_width + forecast_width):\n",
    "        \n",
    "        now = now+timedelta(hours=1) \n",
    "        labels.append(now)\n",
    "\n",
    "    #Registring dates as labels to the plot\n",
    "    inputs_labels = labels[start_input:end_input]\n",
    "    predictions_labels = labels[start_forecast:end_forecast]\n",
    "\n",
    "    #Converting inputs to tensor\n",
    "    inputs = tf.convert_to_tensor(df[start_input:end_input])\n",
    "    inputs_to_model = tf.expand_dims(inputs,axis=0)\n",
    "\n",
    "    #Converting real values to tensor\n",
    "    real_values = tf.convert_to_tensor(df[start_forecast:end_forecast])\n",
    "    \n",
    "\n",
    "    #slicing inputs to retrieve only the decomposed windspeeds\n",
    "    sliced_inputs = inputs[:,-len(label_columns):]\n",
    "\n",
    "\n",
    "    #Scale back to the original value range\n",
    "    original_scale_inputs = scale_back(sliced_inputs,min_values,max_values,label_columns)\n",
    "    \n",
    "    #Sum across all decomposed values\n",
    "    summed_original_scale_inputs = tf.math.reduce_sum(original_scale_inputs,axis=1,keepdims=False,name=None)\n",
    "    \n",
    "    #Make predictions\n",
    "    predictions = model(inputs_to_model)\n",
    "    predictions = tf.squeeze(predictions,axis=0)\n",
    "\n",
    "    #Scale predictions and sum all decomposed windspeeds\n",
    "    original_scale_predictions = scale_back(predictions,min_values,max_values,label_columns)\n",
    "    summed_original_scale_predictions = tf.math.reduce_sum(original_scale_predictions,axis=1,keepdims=False,name=None)\n",
    "\n",
    "    #Avg calc\n",
    "\n",
    "    predictions_avg = calc_ws_avg(summed_original_scale_predictions,steps_per_day)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=inputs_labels,\n",
    "                            y=summed_original_scale_inputs,\n",
    "                            mode='lines',\n",
    "                            name='inputs',\n",
    "                            line = dict(color='#2E294E',width=4)\n",
    "                            ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=predictions_labels,\n",
    "                            y=summed_original_scale_predictions,\n",
    "                            mode='lines',\n",
    "                            name='predictions',\n",
    "                            line = dict(color='#D90368',width=4)\n",
    "                            ))\n",
    "\n",
    "    x_0 = inputs_labels[-1]\n",
    "    text_position = x_0 \n",
    "    \n",
    "    fig.add_annotation(\n",
    "        showarrow=False,\n",
    "        text = 'Wind speed daily avg (m/s)',\n",
    "        x = text_position - timedelta(days=1),\n",
    "        y= 14,\n",
    "        font = dict(size = 12, color = 'black')\n",
    "\n",
    "    )\n",
    "    \n",
    "    for day in range(forecast_days):\n",
    "        \n",
    "        text_string = 'day_' + str(day+1) + ': pred = '+ str(predictions_avg[day])   \n",
    "                \n",
    "        fig.add_annotation(\n",
    "            showarrow=False,\n",
    "            text = text_string,\n",
    "            x = text_position + timedelta(hours=12),\n",
    "            y = 14,\n",
    "            font = dict(size = 11, color = 'black'),\n",
    "        )\n",
    "        fig.add_vline(\n",
    "            x = text_position,\n",
    "            line_width = 1.5,\n",
    "            line_dash = 'dash',\n",
    "            line_color = 'black'\n",
    "        )\n",
    "\n",
    "        text_position = text_position + timedelta(days=1)\n",
    "\n",
    "    fig.update_layout(title='[TRS02] Wind speed forecast',\n",
    "                    xaxis_title='',\n",
    "                    yaxis_title='Windspeed (m/s)',\n",
    "                    template = 'plotly_white',\n",
    "                    yaxis = dict(range=[0,15]))\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/e/github/load-forecast/03.results/'\n",
    "\n",
    "model_list = [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file)) and file.endswith(\".keras\")]\n",
    "\n",
    "model_dict = {}\n",
    "temperature = False\n",
    "for model in model_list:\n",
    "    if \"temperature\" in model:\n",
    "        temperature = True\n",
    "    k_nmodes = model.split(\"_\")[-1]\n",
    "    k_nmodes = int(k_nmodes.split(\".\")[0])\n",
    "    model_dict[model] = {\"temperature\":temperature,\n",
    "                         \"k_nmodes\":k_nmodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {'MeanAbsoluteError': MeanAbsoluteError}\n",
    "#lstm_model = load_model(model_path, custom_objects=custom_objects)\n",
    "\n",
    "model = \"/mnt/e/github/load-forecast/03.results/load_forecast_with_temperature_knmodes_10.keras\"\n",
    "#model_path = os.path.join(path,model)\n",
    "lstm_model = load_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing total_load in 10 nmodes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k_nmodes = int(model.split(\"_\")[-1].split(\".\")[0])\n",
    "\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "df = ciclical_time_encoding(df = df)\n",
    "datetime_range = retrieve_and_remove_datetime(df = df)\n",
    "df = decompose_series_vmd(df = df, timeseries = 'total_load', k_nmodes = k_nmodes)\n",
    "\n",
    "# Columns forecasted\n",
    "label_columns = df.columns.to_list()[7:]\n",
    "\n",
    "temperature = True\n",
    "if temperature == False:\n",
    "    df.drop(columns = ['temperature'], inplace = True)\n",
    "\n",
    "total_load = df['total_load'].to_list()\n",
    "df.drop(columns = ['total_load'], inplace = True)\n",
    "df.drop(columns = ['index'], inplace = True)\n",
    "\n",
    "scal_df, min_max_values = split_and_scal_df(df = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[57441.105 55543.88  53456.535 53479.312 54661.78  55643.246 58365.836\n",
      " 60952.664 62958.742 64536.926 65658.17  66256.71  66042.914 67346.19\n",
      " 67823.47  69181.32  70673.45  71265.555 72565.11  73198.75  72955.07\n",
      " 71259.5   68009.13  65049.555 61609.297 58824.43  56839.02  55598.344\n",
      " 55548.54  56278.902 57738.484 59786.04  61997.484 62820.27  64370.023\n",
      " 64693.62  65588.31  67085.695 68425.336 70718.52  72797.086 74055.336\n",
      " 76364.56  77019.484 75882.805 74677.21  72084.24  68347.086 64653.914\n",
      " 61583.516 59003.72  57570.867 56855.105 57775.344 59031.73  60709.613\n",
      " 61951.453 62938.836 64230.02  64038.477 65292.43  65920.72  67961.92\n",
      " 68883.49  70955.83  72796.18  74200.71  75210.44  74907.95  73871.67\n",
      " 71650.86  67827.24  65058.555 61742.836 59392.23  57899.586 57953.164\n",
      " 58942.49  61024.88  63487.098 66427.82  67617.63  69123.86  70533.43\n",
      " 71558.8   73248.9   75052.08  76422.7   77643.37  78796.19  79235.27\n",
      " 79642.75  78615.336 76494.47  73735.06  70346.4   65941.36  63706.664\n",
      " 60605.023 60612.812 60623.055 61759.906 63056.57  64819.05  66129.64\n",
      " 67570.99  68694.96  69846.87  70645.73  72012.69  72688.46  74099.17\n",
      " 75383.44  76673.1   78134.945 78174.1   77603.78  75347.1   71993.79\n",
      " 68066.64  64487.355 61140.    58787.82  57940.777 57128.734 58101.3\n",
      " 58954.44  60188.625 60911.92  61985.133 62134.777 62347.23  62867.984\n",
      " 63253.152 64347.047 66152.61  67728.586 69305.84  71084.58  72024.31\n",
      " 71197.7   70138.555 67409.61  64420.555 61753.695 59448.03  56906.363\n",
      " 55852.715 55203.098 55095.227 55342.707 55674.13  56079.11  56398.023\n",
      " 55757.496 55550.395 55725.86  56647.723 58097.13  59749.758 62149.33\n",
      " 64885.98  66839.78  68555.31  68943.74  67954.36  66062.79  63166.695], shape=(168,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inputs_to_model = process_input_data(start_input = 0,df=scal_df,input_days=3,\n",
    "                                     forecast_days=5,steps_per_day=24)\n",
    "\n",
    "predictions = model_forecast(inputs_to_model = inputs_to_model,\n",
    "                             model = lstm_model)\n",
    "\n",
    "#Scale predictions and sum all decomposed series\n",
    "original_scale_predictions = scale_back(scaled_values = predictions,\n",
    "                                        min_values = min_max_values[0],\n",
    "                                        max_values = min_max_values[1],\n",
    "                                        label_columns = label_columns)\n",
    "\n",
    "summed_original_scale_predictions = tf.math.reduce_sum(original_scale_predictions,axis=1,keepdims=False,name=None)\n",
    "\n",
    "print(summed_original_scale_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [120] vs. [168] [Op:Sub] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtotal_load\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m72\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m192\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msummed_original_scale_predictions\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/conda3.9/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/conda3.9/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [120] vs. [168] [Op:Sub] name: "
     ]
    }
   ],
   "source": [
    "total_load[72:192] - summed_original_scale_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_dict.keys:\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"Initializing model validation\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    model_path = os.path.join(path,model)\n",
    "    lstm_model = load_model(model_path)\n",
    "\n",
    "    print(f\"Model {model} loaded\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    temperature = model['temperature']\n",
    "    k_nmodes = model['k_nmodes']\n",
    "\n",
    "    print(f\"Initializing feature eng.\")\n",
    "\n",
    "    df = df_raw.copy()\n",
    "    df = ciclical_time_encoding(df = df)\n",
    "    datetime_range = retrieve_and_remove_datetime(df = df)\n",
    "    df = decompose_series_vmd(df = df, timeseries = 'total_load', k_nmodes = k_nmodes)\n",
    "\n",
    "    if temperature == False:\n",
    "        df.drop(columns = ['temperature'], inplace = True)\n",
    "    \n",
    "\n",
    "    total_load = df['total_load'].to_list()\n",
    "    df.drop(columns = ['total_load'], inplace = True)\n",
    "    df.drop(columns = ['index'], inplace = True)\n",
    "\n",
    "    scal_df, min_max_values = split_and_scal_df(df = df)\n",
    "\n",
    "    validation_dict = {}\n",
    "\n",
    "    for row in scal_df.index():\n",
    "\n",
    "        input = scal_df[row:row+72]\n",
    "\n",
    "        output = lstm_model(input)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws-forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
