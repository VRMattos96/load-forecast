{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "from datetime import timedelta,date\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import MeanSquaredError, Huber,MeanAbsoluteError\n",
    "from keras.layers import Dense, LSTM, Reshape,Dropout,Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from vmdpy import VMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'load_forecast.keras'\n",
    "lstm_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_years = [2021,2022]\n",
    "aux_list = []\n",
    "for year in list_years:\n",
    "    df_read = pd.read_csv(\"CURVA_CARGA_\"+str(year)+\".csv\", sep = \";\")\n",
    "    aux_list.append(df_read)\n",
    "    \n",
    "df_load = pd.concat(aux_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vmd_timeseries(time_series_value):\n",
    "    \n",
    "    alpha = 2000     # moderate bandwidth constraint  \n",
    "    tau = 0           # noise-tolerance (no strict fidelity enforcement)  \n",
    "    k_nmodes = 40             # n modes  \n",
    "    DC = 0             # no DC part imposed  \n",
    "    init = 0           # initialize omegas uniformly  \n",
    "    tol = 1e-6\n",
    "    u, u_hat, omega = VMD(time_series_value,alpha,tau,k_nmodes,DC,init,tol)\n",
    "\n",
    "    return(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_back(scaled_values,val_min,val_max,label_columns,variable = 'windspeed'):\n",
    "\n",
    "        if variable == 'windspeed':       \n",
    "                min_values = val_min.tail(len(label_columns)).values\n",
    "                max_values = val_max.tail(len(label_columns)).values\n",
    "                \n",
    "        elif variable == 'val_loss':\n",
    "                min_values = val_min.head(1).values\n",
    "                max_values = val_max.head(1).values\n",
    "\n",
    "        scaled_back = scaled_values * (max_values - min_values) + min_values\n",
    "\n",
    "        return scaled_back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(df_load):\n",
    "\n",
    "    df_load = df_load.rename(columns = {'din_instante': 'datetime',\n",
    "                          'nom_subsistema':'sub',\n",
    "                          'val_cargaenergiahomwmed':'carga_sub',\n",
    "                          'id_subsistema':'id_sub'} )\n",
    "    \n",
    "    df_load = df_load.pivot_table(index = [\"datetime\"], columns=[\"id_sub\"], values = \"carga_sub\")\n",
    "    df_load.reset_index(inplace = True)\n",
    "\n",
    "    #df_load.drop(\"id_sub\", axis =1, inplace = True)\n",
    "\n",
    "    df_load['datetime'] = pd.to_datetime(df_load['datetime'])\n",
    "\n",
    "    #Summing all coluns to reach total load\n",
    "    sub_markets =  df_load.select_dtypes(include=['float64', 'int64']).columns.to_list()\n",
    "    df_load['total_load'] = df_load[sub_markets].sum(axis = 1)\n",
    "\n",
    "    df_load.drop(columns = sub_markets, axis = 1, inplace = True)\n",
    "\n",
    "    #Ciclical encoding datetime object:\n",
    "    day = 1\n",
    "    year = day*365\n",
    "\n",
    "    #hourly data, each day has 24 timesteps \n",
    "    steps_per_day = 24\n",
    "\n",
    "    #Convert into an number\n",
    "    timestamp_s = df_load['datetime'].map(datetime.datetime.timestamp)\n",
    "\n",
    "    #Ciclical enconding for sin and cos\n",
    "    df_load = df_load.assign(year_sin = (np.sin(timestamp_s * (2*np.pi/year))).values)\n",
    "    df_load = df_load.assign(year_cos = (np.cos(timestamp_s * (2*np.pi/year))).values)\n",
    "    df_load = df_load.assign(day_sin = (np.sin(timestamp_s * (2*np.pi/day))).values)\n",
    "    df_load = df_load.assign(day_cos = (np.cos(timestamp_s * (2*np.pi/day))).values)\n",
    "\n",
    "    #Save the dates into another variable and remove it from the original dataframe\n",
    "    saved_dates = df_load['datetime']\n",
    "    saved_dates = saved_dates.reset_index(drop=True)\n",
    "\n",
    "    df_load.drop('datetime', axis = 1, inplace = True)\n",
    "\n",
    "    #Applying Variational Mode Decomposition into the wind speed values:\n",
    "    #The objective is to decompose de original wind speed into K series\n",
    "    #In which the sum of those K series are equal to the original wind speed value for each time step\n",
    "\n",
    "    total_load = df_load['total_load'].values\n",
    "    u = vmd_timeseries(total_load)\n",
    "\n",
    "    #Iterate trought the decomposed wind speed array (u) and append its values to a column in the df_wind dataframe\n",
    "    label_columns = []\n",
    "\n",
    "    for i in range(len(u)):\n",
    "        col_name = 'load_dec_' + str(i+1)\n",
    "        df_load[col_name] = u[i]  \n",
    "        label_columns.append('load_dec_'+str(i+1))\n",
    "\n",
    "    #real_windspeed = df_wind['wind'].values\n",
    "\n",
    "    df_load = df_load.drop(['total_load'],axis = 1)\n",
    "\n",
    "    n = len(df_load)\n",
    "\n",
    "    # Split 70:20:10 (train:validation:test)\n",
    "    # train_df = df_load[0:int(n*0.99)]\n",
    "    # val_df = df_load[int(n*0.7):int(n*0.9)]\n",
    "    # test_df = df_load[int(n*0.9):]\n",
    "\n",
    "    #define batch training size\n",
    "    batch_size = 32\n",
    "\n",
    "    #retrieve the max and min values of each feature. This will be used to bring those values back to the original\n",
    "    #dimesion after scalling it to train the model\n",
    "    min_values = df_load.min()\n",
    "    max_values = df_load.max()\n",
    "\n",
    "    #Scaling the dataframes to train\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df_load)\n",
    "\n",
    "    scal_df = df_load.copy()\n",
    "    scal_df[scal_df.columns] = scaler.transform(df_load[df_load.columns])\n",
    "\n",
    "    return(scal_df,min_values,max_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_processing(df_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws-forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
