{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 22:49:32.472357: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-15 22:49:32.472487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-15 22:49:32.572207: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import MeanSquaredError, Huber,MeanAbsoluteError\n",
    "from keras.layers import Dense, LSTM, Reshape,Dropout,Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "from vmdpy import VMD\n",
    "\n",
    "from typing import List as List, Tuple as Tuple, Dict as Dict,Optional as Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temperature</th>\n",
       "      <th>total_load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4344</th>\n",
       "      <td>2022-07-01 00:00:00+00:00</td>\n",
       "      <td>18.143443</td>\n",
       "      <td>61874.013017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4345</th>\n",
       "      <td>2022-07-01 01:00:00+00:00</td>\n",
       "      <td>17.446575</td>\n",
       "      <td>59042.531005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>2022-07-01 02:00:00+00:00</td>\n",
       "      <td>16.976438</td>\n",
       "      <td>57149.292992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>2022-07-01 03:00:00+00:00</td>\n",
       "      <td>16.481543</td>\n",
       "      <td>56519.531943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>2022-07-01 04:00:00+00:00</td>\n",
       "      <td>16.140110</td>\n",
       "      <td>56818.247011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      datetime  temperature    total_load\n",
       "4344 2022-07-01 00:00:00+00:00    18.143443  61874.013017\n",
       "4345 2022-07-01 01:00:00+00:00    17.446575  59042.531005\n",
       "4346 2022-07-01 02:00:00+00:00    16.976438  57149.292992\n",
       "4347 2022-07-01 03:00:00+00:00    16.481543  56519.531943\n",
       "4348 2022-07-01 04:00:00+00:00    16.140110  56818.247011"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_load = pd.read_csv(\"/mnt/e/github/load-forecast/01.database/processed/load/2022_2023_load_processed.csv\", sep = \",\", encoding = 'latin-1', index_col=0)\n",
    "df_temp = pd.read_csv(\"/mnt/e/github/load-forecast/01.database/processed/temperature/2022_2023_temperature_processed.csv\", sep = ',',encoding = 'latin-1', index_col=0)\n",
    "\n",
    "df_temp['datetime'] = pd.to_datetime(df_temp['datetime'])\n",
    "df_load['datetime'] = pd.to_datetime(df_load['datetime']).dt.tz_localize('UTC')\n",
    "\n",
    "df_raw = pd.merge(df_temp,df_load, on='datetime', how='inner')\n",
    "\n",
    "del df_load\n",
    "del df_temp\n",
    "\n",
    "df_raw = df_raw.iloc[4344:]\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ciclical_time_encoding(df:pd.DataFrame)->pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Encodes datetime values in a DataFrame using cyclic encoding.\n",
    "\n",
    "    Args:\n",
    "        - df: DataFrame containing a 'datetime' column to be cyclically encoded.\n",
    "\n",
    "    Returns:\n",
    "        - DataFrame with additional columns representing the cyclically encoded datetime values.\n",
    "    \"\"\"\n",
    "\n",
    "    #Ciclical encoding datetime object:\n",
    "    day = 1\n",
    "    weekly = day*7\n",
    "    year = day*365\n",
    "\n",
    "    #Convert into an number\n",
    "    timestamp_s = df['datetime'].map(datetime.timestamp)\n",
    "\n",
    "    df = df.assign(year_sin = (np.sin(timestamp_s * (2*np.pi/year))).values)\n",
    "    df = df.assign(year_cos = (np.cos(timestamp_s * (2*np.pi/year))).values)\n",
    "\n",
    "    df = df.assign(daily_sin = (np.sin(timestamp_s * (2*np.pi/day))).values)\n",
    "    df = df.assign(daily_cos = (np.cos(timestamp_s * (2*np.pi/day))).values)\n",
    "\n",
    "    df = df.assign(weekly_sin = (np.sin(timestamp_s * (2*np.pi/weekly))).values)\n",
    "    df = df.assign(weekly_cos = (np.cos(timestamp_s * (2*np.pi/weekly))).values)\n",
    "    \n",
    "    df.reset_index(inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_remove_datetime(df:pd.DataFrame)-> List[datetime]:\n",
    "\n",
    "    '''\n",
    "    - Args:\n",
    "        - df: Dataframe to remove datetime column\n",
    "    - Returns:\n",
    "        - datetime_rage: Removed column as a List\n",
    "    '''\n",
    "   \n",
    "    datetime_range = df['datetime'].to_list()\n",
    "    df.drop(columns = ['datetime'], inplace = True)\n",
    "    \n",
    "    return datetime_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_series_vmd(df:pd.DataFrame,timeseries:str, k_nmodes:int)-> pd.DataFrame:\n",
    "    '''\n",
    "\n",
    "    Decomposes a time series using Variational Mode Decomposition (VMD).\n",
    "    \n",
    "    - Args:\n",
    "        - df: Dataframe with the timeseries that will be decomposed\n",
    "        - timeseres: Dataframe column that will go trought the vmd process\n",
    "        - k_nmodes: n decomposed series\n",
    "    - Returns:\n",
    "        - df: dataframe with new decomposed columns\n",
    "    '''\n",
    "\n",
    "    print(f\"Decomposing {timeseries} in {k_nmodes} nmodes\")\n",
    "    \n",
    "    timeseries = df[timeseries].to_list()\n",
    "\n",
    "    alpha = 2000     # moderate bandwidth constraint  \n",
    "    tau = 0           # noise-tolerance (no strict fidelity enforcement)  \n",
    "    k_nmodes = k_nmodes           # n modes  \n",
    "    DC = 0             # no DC part imposed  \n",
    "    init = 0           # initialize omegas uniformly  \n",
    "    tol = 1e-6\n",
    "    u, u_hat, omega = VMD(timeseries,alpha,tau,k_nmodes,DC,init,tol)\n",
    "\n",
    "    label_columns = []\n",
    "\n",
    "    for i in range(len(u)):\n",
    "        col_name = 'series_dec_' + str(i+1)\n",
    "        df[col_name] = u[i]  \n",
    "        label_columns.append('series_dec_'+str(i+1))\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_scal_df(df:pd.DataFrame)-> Tuple[Dict[str,pd.DataFrame],Tuple[pd.Series,pd.Series]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Splits the input DataFrame into training, validation, and test sets, scales the datasets using Min-Max scaling,\n",
    "    and returns the scaled DataFrames along with the min-max values used for scaling.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame to be split and scaled.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing:\n",
    "        - A dictionary with scaled DataFrams\n",
    "        - A tuple containing min and max values for each feature used during scaling.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(df)\n",
    "    # Split 70:20:10 (train:validation:test)\n",
    "    train_df = df[0:int(n*0.7)]\n",
    "    val_df = df[int(n*0.7):int(n*0.9)]\n",
    "    test_df = df[int(n*0.9):]\n",
    "\n",
    "    #retrieve the max and min values of each feature. This will be used to bring those values back to the original\n",
    "    #dimesion after scalling it to train the model\n",
    "    min_values = train_df.min()\n",
    "    max_values = train_df.max()\n",
    "\n",
    "    #Scaling the dataframes to train\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_df)\n",
    "\n",
    "    scal_df = df.copy()\n",
    "    scal_df[scal_df.columns] = scaler.transform(df[df.columns])\n",
    "\n",
    "    scal_train_df = train_df.copy()\n",
    "    scal_train_df[scal_train_df.columns] = scaler.transform(train_df[train_df.columns])\n",
    "\n",
    "    scal_val_df = val_df.copy()\n",
    "    scal_val_df[scal_val_df.columns] = scaler.transform(val_df[val_df.columns])\n",
    "\n",
    "    scal_test_df = test_df.copy()\n",
    "    scal_test_df[scal_test_df.columns] = scaler.transform(test_df[test_df.columns])\n",
    "\n",
    "    dict_scal_df = {\"train_df\":scal_train_df,\n",
    "                    \"val_df\":scal_val_df,\n",
    "                    \"test_df\":scal_test_df}\n",
    "\n",
    "    min_max_values = (min_values,max_values)\n",
    "\n",
    "    return scal_df,min_max_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_back(scaled_values:tf.Tensor,min_values:pd.Series,max_values:pd.Series,label_columns:List[str],variable = 'timeseries')->tf.Tensor:\n",
    "        \"\"\"\n",
    "        Reverts the scaling transformation applied to TensorFlow tensor during data preprocessing.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        - scaled_values: The scaled TensorFlow tensor to be transformed back to its original scale.\n",
    "        - min_values: Series containing the minimum values used during scaling.\n",
    "        - max_values: Series containing the maximum values used during scaling.\n",
    "        - label_columns: List of column names to consider during scaling.\n",
    "        - variable: String indicating the variable type, either 'timeseries' or 'val_loss'.\n",
    "\n",
    "        Returns:\n",
    "         -----------\n",
    "        - Scaled tensor transformed back to its original scale.\n",
    "        \"\"\"\n",
    "\n",
    "        if variable == 'timeseries':       \n",
    "                min_values = min_values.tail(len(label_columns)).values\n",
    "                max_values = max_values.tail(len(label_columns)).values\n",
    "                \n",
    "        elif variable == 'val_loss':\n",
    "                min_values = min_values.head(1).values\n",
    "                max_values = max_values.head(1).values\n",
    "\n",
    "        scaled_back = scaled_values * (max_values - min_values) + min_values\n",
    "\n",
    "        return scaled_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_data(start_input:int,df:pd.DataFrame,input_days:int,forecast_days:int,steps_per_day:int)-> tf.Tensor:\n",
    "\n",
    "    \"\"\"\n",
    "    Prepares input data for a TensorFlow model by selecting a time window from the DataFrame,\n",
    "    converting it to a tensor, and expanding its dimensions for compatibility with the model.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    - start_input: The starting index of the time window.\n",
    "    - df: The DataFrame containing the input data.\n",
    "    - input_days: Number of days to include in the input window.\n",
    "    - forecast_days: Number of days for forecasting.\n",
    "    - steps_per_day: Number of steps (time intervals) per day.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    - TensorFlow tensor containing the input data, prepared for model prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    #Convert varibles from days to timesteps.\n",
    "    input_hours = input_days*steps_per_day\n",
    "    forecast_hours = forecast_days*steps_per_day\n",
    "\n",
    "    #Input Period\n",
    "    end_input = start_input + input_hours\n",
    "    \n",
    "    #Forecast/Label period\n",
    "    start_forecast = end_input\n",
    "    end_forecast = start_forecast + forecast_hours\n",
    "    \n",
    "    #Converting inputs to tensor\n",
    "    inputs = tf.convert_to_tensor(df[start_input:end_input])\n",
    "    inputs_to_model = tf.expand_dims(inputs,axis=0)\n",
    "    \n",
    "    return(inputs_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forecast(inputs_to_model:tf.Tensor,\n",
    "                   model:Model) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Generates model predictions for the given input using the specified TensorFlow model.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    - inputs_to_model: Tensor, input data for the model.\n",
    "    - model: TensorFlow Model, the trained model for making predictions.\n",
    "\n",
    "    Returns:\n",
    "    ---------\n",
    "    - Tensor, model predictions for the input data.\n",
    "    \"\"\"\n",
    "    #Make predictions\n",
    "    predictions = model(inputs_to_model)\n",
    "    predictions = tf.squeeze(predictions,axis=0)\n",
    "    \n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_validate_predictions(predictions:tf.Tensor,\n",
    "                                     real_values:List[float],\n",
    "                                     min_values:pd.Series,\n",
    "                                     max_values:pd.Series,\n",
    "                                     label_columns:List[str],\n",
    "                                     start_input:int,\n",
    "                                     input_days:int,\n",
    "                                     forecast_days:int,\n",
    "                                     steps_per_day:int,\n",
    "                                     validation_dict:Dict[str,List]):\n",
    "    \"\"\"\n",
    "    Processes model predictions, calculates errors, and updates a validation dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    - predictions: TensorFlow Tensor, model predictions.\n",
    "    - real_values: List of floats, actual values for validation.\n",
    "    - min_values: Pandas Series, minimum values used for scaling.\n",
    "    - max_values: Pandas Series, maximum values used for scaling.\n",
    "    - label_columns: List of strings, columns used for scaling.\n",
    "    - start_input: int, starting point for input data.\n",
    "    - input_days: int, number of days in the input data.\n",
    "    - forecast_days: int, number of days to forecast.\n",
    "    - steps_per_day: int, number of time steps per day.\n",
    "    - validation_dict: Dictionary, storing validation results.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    - Updated validation_dict.\n",
    "    \"\"\"\n",
    "        \n",
    "    #Convert varibles from days to timesteps.\n",
    "    input_hours = input_days*steps_per_day\n",
    "    forecast_hours = forecast_days*steps_per_day\n",
    "\n",
    "    #Input Period\n",
    "    end_input = start_input + input_hours\n",
    "    \n",
    "    #Forecast/Label period\n",
    "    start_forecast = end_input\n",
    "    end_forecast = start_forecast + forecast_hours\n",
    "\n",
    "    #Scale predictions and sum all decomposed series\n",
    "    original_scale_predictions = scale_back(scaled_values = predictions,min_values = min_values, max_values = max_values, label_columns = label_columns)\n",
    "    \n",
    "    summed_original_scale_predictions = tf.math.reduce_sum(original_scale_predictions,axis=1,keepdims=False,name=None)\n",
    "\n",
    "    # Convert 'real_values' to a tensor\n",
    "    real_values = tf.constant(real_values, dtype=tf.float32)\n",
    "\n",
    "    # 'error' is a TensorFlow tensor\n",
    "    error = real_values[start_forecast:end_forecast] - summed_original_scale_predictions\n",
    "\n",
    "    # Split the tensors into sublists of size 24\n",
    "    sublists_error = tf.split(error, num_or_size_splits= forecast_days)\n",
    "    sublists_total_load = tf.split(real_values[start_forecast:end_forecast], num_or_size_splits=forecast_days)\n",
    "\n",
    "    # Calculate the average and standard deviation for each sublist\n",
    "    averages_error = []\n",
    "    std_devs_error = []\n",
    "    averages_total_load = []\n",
    "    std_devs_total_load = []\n",
    "\n",
    "    for sublist_error, sublist_total_load in zip(sublists_error, sublists_total_load):\n",
    "        avg_error = tf.reduce_mean(sublist_error)\n",
    "        std_dev_error = tf.math.reduce_std(sublist_error)\n",
    "        averages_error.append(avg_error)\n",
    "        std_devs_error.append(std_dev_error)\n",
    "\n",
    "        avg_total_load = tf.reduce_mean(sublist_total_load)\n",
    "        std_dev_total_load = tf.math.reduce_std(sublist_total_load)\n",
    "        averages_total_load.append(avg_total_load)\n",
    "        std_devs_total_load.append(std_dev_total_load)\n",
    "\n",
    "        \n",
    "\n",
    "    # Convert the results to numpy arrays for easier handling\n",
    "    averages_error = np.array(averages_error)\n",
    "    std_devs_error = np.array(std_devs_error)\n",
    "    averages_total_load = np.array(averages_total_load)\n",
    "    std_devs_total_load = np.array(std_devs_total_load)\n",
    "\n",
    "    percentual_error = avg_error/averages_total_load\n",
    "    percentual_error = np.array(percentual_error)\n",
    "\n",
    "    for key in validation_dict.keys():\n",
    "\n",
    "        day = key.split(\"_\")[-1]\n",
    "        index_day = int(day) - 1\n",
    "\n",
    "        if key.startswith(\"avg\"):\n",
    "            validation_dict[key].append(averages_error[index_day])\n",
    "        else:\n",
    "            validation_dict[key].append(percentual_error[index_day])\n",
    "\n",
    "    return(validation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/e/github/load-forecast/03.results/'\n",
    "custom_objects = {'MeanAbsoluteError': MeanAbsoluteError}\n",
    "\n",
    "model_list = [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file)) and file.endswith(\".keras\")]\n",
    "model_list = model_list[1:]\n",
    "model_dict = {}\n",
    "\n",
    "for model in model_list:\n",
    "\n",
    "    k_nmodes = model.split(\"_\")[-1]\n",
    "    k_nmodes = int(k_nmodes.split(\".\")[0])\n",
    "    model_dict[model] = k_nmodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "Initializing model validation\n",
      "---------------------------------------------------\n",
      "Model load_forecast_with_temperature_knmodes_10.keras loaded\n",
      "---------------------------------------------------\n",
      "Initializing feature eng.\n",
      "---------------------------------------------------\n",
      "Decomposing total_load in 10 nmodes\n",
      "Initiating load_forecast_with_temperature_knmodes_10.keras validation\n",
      "---------------------------------------------------\n",
      "ERROR:tensorflow:==================================                  \n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f6d71a8b5e0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/vmattos/anaconda3/envs/conda3.9/lib/python3.9/site-packages/keras/src/backend.py\", line 5159, in <genexpr>\n",
      "    ta.write(ta_index_to_write, out)  File \"/home/vmattos/anaconda3/envs/conda3.9/lib/python3.9/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [144] vs. [168] [Op:Sub] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 72\u001b[0m\n\u001b[1;32m     68\u001b[0m inputs_to_model \u001b[38;5;241m=\u001b[39m process_input_data(start_input \u001b[38;5;241m=\u001b[39m row, df \u001b[38;5;241m=\u001b[39m scal_df, input_days \u001b[38;5;241m=\u001b[39m INPUT_DAYS, forecast_days \u001b[38;5;241m=\u001b[39m FORECAST_DAYS, steps_per_day\u001b[38;5;241m=\u001b[39m STEPS_PER_DAY)\n\u001b[1;32m     70\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model_forecast(inputs_to_model \u001b[38;5;241m=\u001b[39m inputs_to_model,model \u001b[38;5;241m=\u001b[39m lstm_model)\n\u001b[0;32m---> 72\u001b[0m model_validation_dict[model] \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_and_validate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mreal_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mmin_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mmax_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mlabel_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mstart_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43minput_days\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mINPUT_DAYS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mforecast_days\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFORECAST_DAYS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43msteps_per_day\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSTEPS_PER_DAY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mvalidation_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_validation_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m days_count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[12], line 52\u001b[0m, in \u001b[0;36mprocess_and_validate_predictions\u001b[0;34m(predictions, real_values, min_values, max_values, label_columns, start_input, input_days, forecast_days, steps_per_day, validation_dict)\u001b[0m\n\u001b[1;32m     49\u001b[0m real_values \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(real_values, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# 'error' is a TensorFlow tensor\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43mreal_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_forecast\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_forecast\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msummed_original_scale_predictions\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Split the tensors into sublists of size 24\u001b[39;00m\n\u001b[1;32m     55\u001b[0m sublists_error \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msplit(error, num_or_size_splits\u001b[38;5;241m=\u001b[39m forecast_days)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda3.9/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/conda3.9/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [144] vs. [168] [Op:Sub] name: "
     ]
    }
   ],
   "source": [
    "from settings import FORECAST_DAYS,INPUT_DAYS,STEPS_PER_DAY\n",
    "\n",
    "validation_dict = {'avg_day_1':[],\n",
    "                'avg_day_2':[],\n",
    "                'avg_day_3':[],\n",
    "                'avg_day_4':[],\n",
    "                'avg_day_5':[],\n",
    "                'avg_day_6':[],\n",
    "                'avg_day_7':[],\n",
    "                'p_avg_day_1':[],\n",
    "                'p_avg_day_2':[],\n",
    "                'p_avg_day_3':[],\n",
    "                'p_avg_day_4':[],\n",
    "                'p_avg_day_5':[],\n",
    "                'p_avg_day_6':[],\n",
    "                'p_avg_day_7':[],\n",
    "                }\n",
    "\n",
    "model_validation_dict = {}\n",
    "\n",
    "for model in model_dict.keys():\n",
    "\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"Initializing model validation\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    model_path = os.path.join(path,model)\n",
    "    lstm_model = load_model(model_path)\n",
    "\n",
    "    print(f\"Model {model} loaded\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    k_nmodes = model_dict[model]\n",
    "\n",
    "    print(f\"Initializing feature eng.\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    \n",
    "    df = df_raw.copy()\n",
    "    df = ciclical_time_encoding(df = df)\n",
    "    datetime_range = retrieve_and_remove_datetime(df = df)\n",
    "    df = decompose_series_vmd(df = df, timeseries = 'total_load', k_nmodes = k_nmodes)\n",
    "\n",
    "    total_load = df['total_load'].to_list()\n",
    "    df.drop(columns = ['total_load'], inplace = True)\n",
    "    df.drop(columns = ['index'], inplace = True)\n",
    "\n",
    "    scal_df, min_max_values = split_and_scal_df(df = df)\n",
    "\n",
    "    # Forecast columns\n",
    "    label_columns = scal_df.columns.to_list()[7:]\n",
    "\n",
    "    #Min and max values\n",
    "    min_values = min_max_values[0]\n",
    "    max_values = min_max_values[1]\n",
    "\n",
    "    model_validation_dict[model] = validation_dict\n",
    "\n",
    "    print(f\"Initiating {model} validation\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    days_count = 1\n",
    "    for row in range(0, len(scal_df), 24):\n",
    "\n",
    "        total_days = int(len(scal_df)/24)\n",
    "\n",
    "        print(f\"Validating day {days_count} | Total progress: {np.round((days_count/total_days)*100,2)}%                          \",end = '\\r')\n",
    "                \n",
    "        inputs_to_model = process_input_data(start_input = row, df = scal_df, input_days = INPUT_DAYS, forecast_days = FORECAST_DAYS, steps_per_day= STEPS_PER_DAY)\n",
    "\n",
    "        predictions = model_forecast(inputs_to_model = inputs_to_model,model = lstm_model)\n",
    "\n",
    "        model_validation_dict[model] = process_and_validate_predictions(predictions=predictions, \n",
    "                                                            real_values = total_load, \n",
    "                                                            min_values = min_values, \n",
    "                                                            max_values = max_values, \n",
    "                                                            label_columns=label_columns, \n",
    "                                                            start_input = row, \n",
    "                                                            input_days = INPUT_DAYS, \n",
    "                                                            forecast_days = FORECAST_DAYS, \n",
    "                                                            steps_per_day = STEPS_PER_DAY, \n",
    "                                                            validation_dict = model_validation_dict[model])\n",
    "        days_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['series_dec_1',\n",
       " 'series_dec_2',\n",
       " 'series_dec_3',\n",
       " 'series_dec_4',\n",
       " 'series_dec_5',\n",
       " 'series_dec_6',\n",
       " 'series_dec_7',\n",
       " 'series_dec_8',\n",
       " 'series_dec_9',\n",
       " 'series_dec_10']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws-forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
